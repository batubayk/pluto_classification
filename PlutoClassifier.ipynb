{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification task on PLUTO (New York City Primary Land Use Tax Lot Output) Dataset\n",
    "\n",
    "Scikit learn, pandas and matplotlib packages of python were chosen. These packages provide various utilities that make preprocessing datasets and training machine learning algorithms much more easier. The preprocessing steps, classification choices, cross validation and hyper-parameter tuning steps are explained in detail. Some sections can be skipped when reproducing results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38569,
     "status": "ok",
     "timestamp": 1556740033342,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "jCcxYbjs15Ta",
    "outputId": "93d888fb-080d-4f7c-eb2a-9ddefbd08f54"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve,f1_score,average_precision_score,auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RANDOM_SEED=42\n",
    "#Read the training and test datasets\n",
    "trainData='train.csv'\n",
    "testData='test.csv'\n",
    "\n",
    "train=pd.read_csv(trainData,header=0).drop(columns='index')\n",
    "test=pd.read_csv(testData,header=0).drop(columns='index')\n",
    "trainSize=len(train)\n",
    "testSize=len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data Column-wise\n",
    "\n",
    "When the data is inspected it can be seen that there are a lot of missing values. Hence, the columns that contain a great portion of missing values are eliminated according to a certain threshold. The threshold is selected emprically as 50 percent after inspecting the columns that have missing values from the data dictionary. In this case, %50 threshold gave reasonable results when the trained model was validated. However, in some cases even columns that have much higher amount of missing values might be valuable in the classification task but in this case the approach was sufficient. Importantly, both the train and test set seems to come from the same distribution and the columns to be dropped are identical so dropping them won't cause a problem on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40496,
     "status": "ok",
     "timestamp": 1556740035286,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "eogs6Ucu15Tk",
    "outputId": "92dfa6e2-0bf3-4628-8a46-88becac27350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "borough                 0.000000\n",
      "block                   0.000000\n",
      "lot                     0.000000\n",
      "schooldist              0.003205\n",
      "council                 0.003516\n",
      "zipcode                 0.004840\n",
      "firecomp                0.004079\n",
      "policeprct              0.003451\n",
      "healthcenterdistrict    0.003093\n",
      "healtharea              0.003854\n",
      "sanitboro               0.007409\n",
      "sanitdistrict           0.007369\n",
      "sanitsub                0.018478\n",
      "zonedist1               0.001173\n",
      "zonedist2               0.976886\n",
      "zonedist3               0.999783\n",
      "zonedist4               0.999984\n",
      "overlay1                0.912974\n",
      "overlay2                0.999818\n",
      "spdist1                 0.881404\n",
      "spdist2                 0.999921\n",
      "spdist3                 1.000000\n",
      "ltdheight               0.996481\n",
      "splitzone               0.001173\n",
      "landuse                 0.003336\n",
      "easements               0.000000\n",
      "lotarea                 0.000000\n",
      "bldgarea                0.000000\n",
      "comarea                 0.000000\n",
      "resarea                 0.000000\n",
      "                          ...   \n",
      "strgearea               0.000000\n",
      "factryarea              0.000000\n",
      "otherarea               0.000000\n",
      "numbldgs                0.000000\n",
      "numfloors               0.000000\n",
      "unitstotal              0.000000\n",
      "lotfront                0.000000\n",
      "lotdepth                0.000000\n",
      "bldgfront               0.000000\n",
      "bldgdepth               0.000000\n",
      "ext                     0.592568\n",
      "proxcode                0.000578\n",
      "irrlotcode              0.000578\n",
      "lottype                 0.000578\n",
      "bsmtcode                0.000578\n",
      "assessland              0.000000\n",
      "assesstot               0.000000\n",
      "exemptland              0.000000\n",
      "exempttot               0.000000\n",
      "yearbuilt               0.000000\n",
      "yearalter1              0.000000\n",
      "yearalter2              0.000000\n",
      "histdist                0.965279\n",
      "landmark                0.998167\n",
      "builtfar                0.000000\n",
      "tract2010               0.000000\n",
      "xcoord                  0.027949\n",
      "ycoord                  0.027949\n",
      "zonemap                 0.001145\n",
      "target__office          0.000000\n",
      "Length: 63, dtype: float64\n",
      "The columns to be dropped from train and test data are the same:  True\n",
      "Train data size:  687369\n",
      "Test data size:  171843\n",
      "Number of columns dropped:  12\n"
     ]
    }
   ],
   "source": [
    "#Print the percentage of NaNs that each column has in training data for analysis\n",
    "print(train.isnull().sum(axis=0)/len(train))\n",
    "\n",
    "#Define a threshold for dropping the columns with NaN percent above a certain threshold\n",
    "threshold=0.5\n",
    "\n",
    "#Drop the columns exceeding the threshold both on training and test data\n",
    "trainColumnsDrop=train.columns[train.isnull().sum(axis=0)/len(train)>threshold]\n",
    "testColumnsDrop=test.columns[test.isnull().sum(axis=0)/len(test)>threshold]\n",
    "\n",
    "print(\"The columns to be dropped from train and test data are the same: \", pd.Index.equals(trainColumnsDrop,testColumnsDrop))\n",
    "\n",
    "train=train.drop(trainColumnsDrop,axis=1)\n",
    "test=test.drop(testColumnsDrop,axis=1)\n",
    "\n",
    "print(\"Train data size: \",trainSize)\n",
    "print(\"Test data size: \",testSize)\n",
    "print(\"Number of columns dropped: \",len(trainColumnsDrop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the NaN values\n",
    "\n",
    "Dropping the columns with high amount of NaN values is not enough and further preprocessing is needed. There are still some amount of missing values left in the data. After, inspecting the data dictionary I found it not very reasonable to fill the missing data according to mean values in the column or by just replacing with the most occuring values. Instead, the numeric columns were filled with -1 (Not within range of any column values) or as \"MISSING\" for string valued columns. Providing the classifier with missing information might even be better in terms training the model since missing information is also valuable when compared to dropping all the values. Importantly, dropping the rows that contained NaN values was also tried but resulted in a great amount of elimination of training data. Hence, instead of dropping NaN, they were filled as missing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50227,
     "status": "ok",
     "timestamp": 1556740045053,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "AezTUKdR15Tp",
    "outputId": "f19c2966-1344-4f3c-d3ab-79b6300e49b3"
   },
   "outputs": [],
   "source": [
    "#Find numeric and categorical columns\n",
    "float_cols_tr = train.select_dtypes(include=['float64','int64']).columns\n",
    "str_cols_tr = train.select_dtypes(include=['object']).columns\n",
    "float_cols_test = test.select_dtypes(include=['float64','int64']).columns\n",
    "str_cols_test = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "#fill the numeric columns with -1 and categorical columns with 'MISSING'\n",
    "train.loc[:, float_cols_tr]=train.loc[:, float_cols_tr].fillna(-1)\n",
    "train.loc[:, str_cols_tr]=train.loc[:, str_cols_tr].fillna('MISSING')\n",
    "test.loc[:, float_cols_test]=test.loc[:, float_cols_test].fillna(-1)\n",
    "test.loc[:, str_cols_test]=test.loc[:, str_cols_test].fillna('MISSING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the values in the dataset\n",
    "\n",
    "Since categorical data is not allowed by most of the classifiers the data is transformed by encoding the labels. Scikit learn only accepts data in numeric format, therefore a very handy utility called LabelEncoder was used. After the encoding operation, training feature and the target value was seperated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>block</th>\n",
       "      <th>lot</th>\n",
       "      <th>schooldist</th>\n",
       "      <th>council</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>firecomp</th>\n",
       "      <th>policeprct</th>\n",
       "      <th>healthcenterdistrict</th>\n",
       "      <th>healtharea</th>\n",
       "      <th>...</th>\n",
       "      <th>exemptland</th>\n",
       "      <th>exempttot</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>yearalter1</th>\n",
       "      <th>yearalter2</th>\n",
       "      <th>builtfar</th>\n",
       "      <th>tract2010</th>\n",
       "      <th>xcoord</th>\n",
       "      <th>ycoord</th>\n",
       "      <th>zonemap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3717</td>\n",
       "      <td>64</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10472.0</td>\n",
       "      <td>69</td>\n",
       "      <td>43.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>48</td>\n",
       "      <td>1018914.0</td>\n",
       "      <td>239801.0</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>15818</td>\n",
       "      <td>48</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11691.0</td>\n",
       "      <td>139</td>\n",
       "      <td>101.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>99802</td>\n",
       "      <td>1051073.0</td>\n",
       "      <td>155952.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6787</td>\n",
       "      <td>7</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11229.0</td>\n",
       "      <td>147</td>\n",
       "      <td>61.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7320.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1470</td>\n",
       "      <td>1470</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>548</td>\n",
       "      <td>998378.0</td>\n",
       "      <td>161944.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>672</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11232.0</td>\n",
       "      <td>108</td>\n",
       "      <td>72.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1910</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>101</td>\n",
       "      <td>983777.0</td>\n",
       "      <td>178952.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6651</td>\n",
       "      <td>7501</td>\n",
       "      <td>21.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>11223.0</td>\n",
       "      <td>195</td>\n",
       "      <td>62.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8522.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>356862</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.59</td>\n",
       "      <td>426</td>\n",
       "      <td>990141.0</td>\n",
       "      <td>159965.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1583</td>\n",
       "      <td>151</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11373.0</td>\n",
       "      <td>158</td>\n",
       "      <td>110.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>471</td>\n",
       "      <td>1018443.0</td>\n",
       "      <td>209795.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>8538</td>\n",
       "      <td>9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11040.0</td>\n",
       "      <td>129</td>\n",
       "      <td>105.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1470</td>\n",
       "      <td>1470</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>155102</td>\n",
       "      <td>1064643.0</td>\n",
       "      <td>212705.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7427</td>\n",
       "      <td>41</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>11235.0</td>\n",
       "      <td>188</td>\n",
       "      <td>61.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>626</td>\n",
       "      <td>1002246.0</td>\n",
       "      <td>155099.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>5521</td>\n",
       "      <td>85</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10465.0</td>\n",
       "      <td>50</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>118</td>\n",
       "      <td>1037553.0</td>\n",
       "      <td>236606.0</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>5767</td>\n",
       "      <td>791</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10463.0</td>\n",
       "      <td>243</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>285</td>\n",
       "      <td>1011053.0</td>\n",
       "      <td>261816.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   borough  block   lot  schooldist  council  zipcode  firecomp  policeprct  \\\n",
       "0        1   3717    64         8.0     18.0  10472.0        69        43.0   \n",
       "1        3  15818    48        27.0     31.0  11691.0       139       101.0   \n",
       "2        0   6787     7        22.0     48.0  11229.0       147        61.0   \n",
       "3        0    672    30        15.0     38.0  11232.0       108        72.0   \n",
       "4        0   6651  7501        21.0     44.0  11223.0       195        62.0   \n",
       "5        3   1583   151        24.0     25.0  11373.0       158       110.0   \n",
       "6        3   8538     9        26.0     23.0  11040.0       129       105.0   \n",
       "7        0   7427    41        22.0     48.0  11235.0       188        61.0   \n",
       "8        1   5521    85         8.0     13.0  10465.0        50        45.0   \n",
       "9        1   5767   791        10.0     11.0  10463.0       243        50.0   \n",
       "\n",
       "   healthcenterdistrict  healtharea   ...     exemptland  exempttot  \\\n",
       "0                  26.0      3020.0   ...              0          0   \n",
       "1                  45.0      3800.0   ...              0          0   \n",
       "2                  35.0      7320.0   ...           1470       1470   \n",
       "3                  39.0      4400.0   ...              0          0   \n",
       "4                  37.0      8522.0   ...              0     356862   \n",
       "5                  42.0      1420.0   ...              0          0   \n",
       "6                  43.0      2170.0   ...           1470       1470   \n",
       "7                  37.0      8900.0   ...              0          0   \n",
       "8                  26.0      3220.0   ...              0          0   \n",
       "9                  21.0       200.0   ...              0          0   \n",
       "\n",
       "   yearbuilt  yearalter1  yearalter2  builtfar  tract2010     xcoord  \\\n",
       "0       1940           0           0      0.62         48  1018914.0   \n",
       "1       1930           0           0      0.48      99802  1051073.0   \n",
       "2       1930           0           0      0.69        548   998378.0   \n",
       "3       1910           0           0      1.24        101   983777.0   \n",
       "4       2002           0           0      3.59        426   990141.0   \n",
       "5       1968           0           0      1.52        471  1018443.0   \n",
       "6       1950           0           0      0.29     155102  1064643.0   \n",
       "7       1955           0           0      1.22        626  1002246.0   \n",
       "8       1930           0           0      0.41        118  1037553.0   \n",
       "9       1940           0           0      0.88        285  1011053.0   \n",
       "\n",
       "     ycoord  zonemap  \n",
       "0  239801.0      113  \n",
       "1  155952.0       88  \n",
       "2  161944.0       57  \n",
       "3  178952.0       25  \n",
       "4  159965.0       55  \n",
       "5  209795.0      128  \n",
       "6  212705.0        7  \n",
       "7  155099.0       78  \n",
       "8  236606.0      117  \n",
       "9  261816.0       43  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the data to a format compatible with scikitlearn. Employ label encoding on each column\n",
    "le=LabelEncoder()\n",
    "train.loc[:, str_cols_tr]=train.loc[:, str_cols_tr].apply(le.fit_transform)\n",
    "test.loc[:, str_cols_test]=test.loc[:, str_cols_test].apply(le.fit_transform)\n",
    "\n",
    "#Seperate each the features from the target class\n",
    "trainX,trainY=train.iloc[:,:-1],train.iloc[:,-1]\n",
    "\n",
    "trainX.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance\n",
    "The most important issue for this dataset is the class imbalance problem. It is easily seen that the majority of the instances in the training set contain a great deal of negative class labels where as the number of positive labels are extremely low. The percentage of negative labels are almost %99.2 where as positive labels are almost %0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50214,
     "status": "ok",
     "timestamp": 1556740045057,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "jsfGKtr-15Tu",
    "outputId": "a67715d8-b134-4189-e043-133727b29f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    681787\n",
      "True       5582\n",
      "Name: target__office, dtype: int64\n",
      "\n",
      "Negative Class Percentage:  99.18791798873676\n",
      "Positive Class Percentage:  0.8120820112632371\n"
     ]
    }
   ],
   "source": [
    "#Imbalanced dataset\n",
    "print(trainY.value_counts())\n",
    "print()\n",
    "print(\"Negative Class Percentage: \",trainY.value_counts()[0]/trainY.value_counts().sum()*100)\n",
    "print(\"Positive Class Percentage: \",trainY.value_counts()[1]/trainY.value_counts().sum()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to see which features explains the variance most ?\n",
    "Principal Component Analysis (PCA) is a very useful tool commonly used to interpret the amount of variance a feature explains in the dataset. It is also commonly used in dimentionality reduction in the case of too many features and to be able to plot such data. In this case, features were mostly categorical therefore applying PCA would not make much sense. I wanted to see if anything meaningful will be obtained after normalizing the data. In this case, most of the features contribute to the variance and the first 10 features explain around %50 of the variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58741,
     "status": "ok",
     "timestamp": 1556740053596,
     "user": {
      "displayName": "batuhan baykara",
      "photoUrl": "",
      "userId": "16124025003563817946"
     },
     "user_tz": -180
    },
    "id": "an4z_4dJ15Ty",
    "outputId": "1ca6f454-00b0-47bf-d2bf-15d5d3fd509f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformedData = StandardScaler().fit_transform(trainX)\n",
    "pca = PCA().fit(transformedData);\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models and checking overfitting and underfitting\n",
    "\n",
    "#### NOTE: This part of the notebook can be skipped when producing the results since it is mostly about figuring out if overfitting occurs or not based on stratified K-fold cross validation !\n",
    "\n",
    "Since the dataset suffers from class imbalance it is useful to employ a classification algorithm that tend to solve this problem to an extent. Random forests is a good option because of bagging and the data won't need much normalization and it tends to do better on imbalaced datasets since it is more robust with decision boundaries. However, other classifiers like weighted linear SVM, naive bayes classifier, stochastic gradient descent classifier and ada boost with decision tree as its classifier were tried. Experiments with these classfiers are commented out since it was decided to proceed with random forest classifier due to better results and efficiency.\n",
    "\n",
    "First experiments were made using the heldout approach where 30 percent of the training data was selected as validation set. Stratified selection was utilized to eliminate any problems due to class imbalance problem. Then, Stratified 3-fold cross validation was applied to see whether our model overfits the data or not. According to the results the model is succesfull on all folds and performs similary hence it seems to fit the data well. \n",
    "\n",
    "Furthermore, the evaluation metric is also a very important factor with datasets that have class imbalance problem. For instance, accuracy measure is misleading since the majority of the class belongs to negative class and would dominate the accuracy measure. Therefore, several approaches were used to successfully measure the performance. A confusion matrix with precision recall and f1 score was utilized. Precision and recall are imoprtant since these are based on positive class. Another metric was precision recall curve which is preferred in datasets that are imbalanced. \n",
    "\n",
    "A custom scorer function was implemented to incorporate these metrics in the evaluation step of the models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "_xM4LTpt15T3",
    "outputId": "6353efc3-98eb-437a-93f3-c82bb88e5d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    204600\n",
      "        True       0.97      0.91      0.94      1611\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    206211\n",
      "   macro avg       0.98      0.95      0.97    206211\n",
      "weighted avg       1.00      1.00      1.00    206211\n",
      "\n",
      "F1-measure=0.937 AUC=0.975 Average Precision=0.974\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    227263\n",
      "        True       0.96      0.90      0.93      1861\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    229124\n",
      "   macro avg       0.98      0.95      0.96    229124\n",
      "weighted avg       1.00      1.00      1.00    229124\n",
      "\n",
      "F1-measure=0.929 AUC=0.971 Average Precision=0.970\n",
      "[CV] ....................... , score=0.9286705523175133, total= 1.3min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    227262\n",
      "        True       0.96      0.90      0.93      1861\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    229123\n",
      "   macro avg       0.98      0.95      0.97    229123\n",
      "weighted avg       1.00      1.00      1.00    229123\n",
      "\n",
      "F1-measure=0.931 AUC=0.974 Average Precision=0.973\n",
      "[CV] ....................... , score=0.9307371349095966, total= 1.3min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    227262\n",
      "        True       0.96      0.90      0.93      1860\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    229122\n",
      "   macro avg       0.98      0.95      0.97    229122\n",
      "weighted avg       1.00      1.00      1.00    229122\n",
      "\n",
      "F1-measure=0.933 AUC=0.973 Average Precision=0.972\n",
      "[CV] ....................... , score=0.9328896283971159, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.0min finished\n"
     ]
    }
   ],
   "source": [
    "def custom_scorer(estimator,X,y):\n",
    "    print(estimator)\n",
    "    yPred=estimator.predict(X)\n",
    "    yProbs=estimator.predict_proba(X)[:,1]\n",
    "   \n",
    "    accuracy=accuracy_score(y,yPred)\n",
    "    precision, recall, thresholds = precision_recall_curve(y, yProbs)\n",
    "    f1 = f1_score(y, yPred)\n",
    "    auc_score = auc(recall, precision)\n",
    "    ap = average_precision_score(y, yProbs)\n",
    "    print(classification_report(y,yPred))\n",
    "    print('F1-measure=%.3f AUC=%.3f Average Precision=%.3f' % (f1, auc_score, ap))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "####################### HELDOUT Random Forest #############################\n",
    "#Stratified sampling is done since the dataset is imbalanced\n",
    "X,valX,y,valY=train_test_split(trainX,trainY,test_size=0.3,random_state=RANDOM_SEED)\n",
    "\n",
    "model=RandomForestClassifier(n_estimators=100,max_depth=50,random_state=RANDOM_SEED)\n",
    "model.fit(X,y)\n",
    "custom_scorer(model,valX,valY)\n",
    "\n",
    "#Linear SVM with class weighting\n",
    "#svm = SVC(kernel='linear', class_weight='balanced', probability=True) \n",
    "#svm.fit(X, y)\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "#gnb = GaussianNB()\n",
    "#gnb.fit(X,y)\n",
    "#custom_scorer(gnb,valX,valY)\n",
    "\n",
    "#Stochastic Gradient Descent Classifier with log loss in order get probabiility based results\n",
    "#clf = SGDClassifier(loss='log')\n",
    "#scaler = StandardScaler().fit(X)\n",
    "#X = scaler.transform(X)\n",
    "#clf.fit(X,y)\n",
    "#valX = scaler.transform(valX)\n",
    "#custom_scorer(clf,valX,valY)\n",
    "\n",
    "#Ada Boost Classsifier with decision trees as classifier\n",
    "#abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "#abc.fit(X,y)\n",
    "#custom_scorer(abc,valX,valY)\n",
    "\n",
    "####################### Stratified K-Fold Random Forest #############################\n",
    "#To make sure not to overfit data utilize k-fold evaluation \n",
    "\n",
    "results = cross_val_score(estimator=model, X=trainX, y=trainY, cv=3,scoring=custom_scorer,verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "#### NOTE: This part of the notebook can be skipped when producing the results since it is mostly about finding the best parameters for the final model !\n",
    "\n",
    "Classifiers usually have various hyperparameters which also effect the performance of the model. These parameters need to be finetuned to come up with a model that performs better than the others. Therefore, a hyperparmeter optimization was done in order to find the best parameters using grid search. The parameter grid was constructed empirically with values that are usually considered to be more important in random forests. Although, there are many hyperparameters such as min sample in leaf, max features, max depth, number of models, splitting criterion and so on. Only, n_estimators and max_depth were considered due to computational time. The best parameters in the end were; num_estimators=200 and max_depth=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLchEaUv15T8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  27 | elapsed:   53.2s remaining: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  27 | elapsed:   54.5s remaining:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  27 | elapsed:   55.9s remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  27 | elapsed:  1.1min remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed:  1.7min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  27 | elapsed:  1.8min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  27 | elapsed:  1.8min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  27 | elapsed:  1.8min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  27 | elapsed:  2.1min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  27 | elapsed:  3.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  27 | elapsed:  3.3min remaining:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:  3.3min remaining:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'max_depth': 50}\n"
     ]
    }
   ],
   "source": [
    "####################### Grid Search for Random Forest classifier ###################\n",
    "param_grid = { \n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth':[50,100,150]\n",
    "#    'criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv= 3,scoring='f1', verbose=20,n_jobs=-1)\n",
    "grid_search.fit(trainX, trainY)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the final model\n",
    "\n",
    "Now that the best parameters for the model have been found with grid search, the final model can be trained on the whole training data. Then the test set can be used to make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlC4zDD915T_"
   },
   "outputs": [],
   "source": [
    "#Predict actual test set and sace data\n",
    "model=RandomForestClassifier(n_estimators=200,max_depth=50,n_jobs=-1,random_state=RANDOM_SEED)\n",
    "model.fit(trainX,trainY)\n",
    "yPredTest=model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the predictions to the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBlgfTUW9png"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See if the predictions are dominated by negative class\n",
      "False    170542\n",
      "True       1301\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Create a new column for predictions\n",
    "pred_column=pd.Series(data=yPredTest)\n",
    "print(\"See if the predictions are dominated by negative class\")\n",
    "print(pred_column.value_counts())\n",
    "\n",
    "pred_column.replace({0: \"FALSE\", 1: \"TRUE\"},inplace=True)\n",
    "\n",
    "#Read the test file again, append the prediction column and save it\n",
    "df = pd.read_csv(testData)\n",
    "df['predicted_office'] = pred_column \n",
    "df.to_csv(testData,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PlutoClassifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
